{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader\n",
    "from surprise import dump\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from surprise import accuracy, Dataset, SVD, KNNBasic, KNNWithMeans, KNNWithZScore,Prediction,SlopeOne\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import Prediction\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leggi Dati e dividi in trainset e testset\n",
    "# DATABASE 1M contenente i rating\n",
    "data = Dataset.load_builtin(\"ml-1m\")\n",
    "\n",
    "raw_ratings = data.raw_ratings\n",
    "\n",
    "# shuffle ratings if you want\n",
    "random.shuffle(raw_ratings)\n",
    "\n",
    "# A = 80% of the data, B = 20% of the data\n",
    "threshold = int(0.8 * len(raw_ratings))\n",
    "A_raw_ratings = raw_ratings[:threshold]\n",
    "B_raw_ratings = raw_ratings[threshold:]\n",
    "\n",
    "data.raw_ratings = A_raw_ratings  # data is now the set A\n",
    "\n",
    "# retrain on the whole set A\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# Compute unbiased accuracy on B\n",
    "testset = data.construct_testset(B_raw_ratings)  # testset is now the set B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leggo i dataset contenenti i film con relativi id,nomi e generi\n",
    "movies_df = pd.read_csv('dataset/ml-1m/movies.dat',\n",
    "                        delimiter='::', engine= 'python', header=None,\n",
    "                        names=['movie_id','movie_name', 'genre'], encoding='latin-1')\n",
    "movies_df.head()\n",
    "\n",
    "#creo un dizionario che collega gli id dei film ai relativi titoli\n",
    "\n",
    "idToName = movies_df.set_index('movie_id').transpose().to_dict(orient='dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    \"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stampa la lista dei film consigliati\n",
    "def mostraListaConsigliati(idUtente,top,idToName):\n",
    "    \"\"\"prints the list top-N recommendation for a user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        idUtente(string): the id of the user.\n",
    "        n(dict): dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "        idToName: dict where keys are movie (raw) ids and values are the movies name\n",
    "\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    i=1\n",
    "    for x in top[idUtente]:\n",
    "        print(i,' Titolo:', idToName.get(int(x[0]))[\"movie_name\"], 'ratingStimato', x[1])\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(\n",
    "            ((true_r >= threshold) and (est >= threshold))\n",
    "            for (est, true_r) in user_ratings[:k]\n",
    "        )\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x22b53d280d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#allenamento modello SVD con iperparametri trovati\n",
    "algo_svd = SVD(n_epochs=30,reg_all=0.05, verbose= False)\n",
    "algo_svd.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump.dump(file_name='modelli_Allenati/SVD',algo=algo_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.knns.KNNBasic at 0x22b53982d90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#allenamento modello knn base user-based con iperparametri trovati\n",
    "algo_knnBase_u = KNNBasic(k=49,sim_options= {'name': 'msd', 'user_based': True}, verbose= False)\n",
    "algo_knnBase_u.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump.dump(file_name='modelli_Allenati/KnnBase_u',algo=algo_knnBase_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allenamento modello knn base user-based con iperparametri trovati\n",
    "algo_knnBase_i = KNNBasic(k=49,sim_options= {'name': 'msd', 'user_based': False}, verbose= False)\n",
    "algo_knnBase_i.fit(trainset)\n",
    "dump.dump(file_name='modelli_Allenati/KnnBase_i',algo=algo_knnBase_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allenamento modello knn base ibrido con iperparametri trovati\n",
    "\n",
    "#costruisco un test-set sull training set\n",
    "biasdtestset=trainset.build_testset()\n",
    "#predicco i rating sul set con i modelli item-based e user-based \n",
    "predictions1 = algo_knnBase_u.test(biasdtestset)\n",
    "predictions2 = algo_knnBase_i.test(biasdtestset)\n",
    "\n",
    "#prendo solo i rating predetti e quelli reali per costruire il training set dell'algoritmo ibrido\n",
    "X=[]\n",
    "y=[]\n",
    "for i in range(len(predictions1)):\n",
    "    X.append([predictions1[i][3],predictions2[i][3]])\n",
    "    y.append(predictions1[i][2])\n",
    "#uso regressione lineare per predirre i rating reali sulla base delle predizioni user-based e item-based\n",
    "algo_KnnBase_ibrido = LinearRegression().fit(X, y)\n",
    "dump.dump(file_name='modelli_Allenati/algo_KnnBase_ibrido',algo=algo_KnnBase_ibrido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funzione che riporta le predizioni del modello ibrido nello stesso formato degli altri\n",
    "def predictIbrido(algo_ibrido,algo_user,algo_item,set):\n",
    "     \"\"\"returns the predictions for thy hybrid model based on those\n",
    "        made by the user-based and item-based models.\n",
    "\n",
    "    Args:\n",
    "        algo_KnnBase_ibrido(string): the trained hybrid model to use.\n",
    "        algo_user: the trained model for user-based predictions to use.\n",
    "        algo_item: the trained model for user-based predictions to use.\n",
    "        set: the set of ratings to predict\n",
    "\n",
    "    Returns:\n",
    "        list of Prediction objects\n",
    "    \"\"\"\n",
    "     predictions1 = algo_user.test(set)\n",
    "     predictions2 = algo_item.test(set)\n",
    "     X=[]\n",
    "     y=[]\n",
    "     for i in range(len(predictions1)):\n",
    "        X.append([predictions1[i][3],predictions2[i][3]])\n",
    "        y.append(predictions1[i][2])\n",
    "    \n",
    "     algo_ibrido.score(X, y)\n",
    "     predictionsIbride=algo_ibrido.predict(X)\n",
    "     predFinali=[]\n",
    "     for i in range(len(predictions1)):\n",
    "        predFinali.append(Prediction(predictions1[i].uid, predictions1[i].iid, predictions1[i].r_ui, predictionsIbride[i], predictions1[i].details))\n",
    "     return predFinali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allenamento modello knn con media user-based con iperparametri trovati\n",
    "algo_knnMeans_u = KNNWithMeans(k=49,sim_options= {'name': 'msd', 'user_based': True}, verbose= False)\n",
    "algo_knnMeans_u.fit(trainset)\n",
    "dump.dump(file_name='modelli_Allenati/algo_knnMeans_u',algo=algo_knnMeans_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allenamento modello knn base user-based con iperparametri trovati\n",
    "algo_knnMeans_i = KNNWithMeans(k=49,sim_options= {'name': 'msd', 'user_based': False}, verbose= False)\n",
    "algo_knnMeans_i.fit(trainset)\n",
    "dump.dump(file_name='modelli_Allenati/algo_knnMeans_i',algo=algo_knnMeans_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allenamento modello knn base ibrido con iperparametri trovati\n",
    "\n",
    "#costruisco un test-set sull training set\n",
    "biasdtestset=trainset.build_testset()\n",
    "#predicco i rating sul set con i modelli item-based e user-based \n",
    "predictions1_m = algo_knnMeans_u.test(biasdtestset)\n",
    "predictions2_m = algo_knnMeans_i.test(biasdtestset)\n",
    "\n",
    "#prendo solo i rating predetti e quelli reali per costruire il training set dell'algoritmo ibrido\n",
    "X=[]\n",
    "y=[]\n",
    "for i in range(len(predictions1_m)):\n",
    "    X.append([predictions1_m[i][3],predictions2_m[i][3]])\n",
    "    y.append(predictions1_m[i][2])\n",
    "#uso regressione lineare per predirre i rating reali sulla base delle predizioni user-based e item-based\n",
    "algo_KnnMeans_ibrido = LinearRegression().fit(X, y)\n",
    "dump.dump(file_name='modelli_Allenati/algo_KnnMeans_ibrido',algo=algo_KnnMeans_ibrido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allenamento modello SlopeOne\n",
    "slopeOne = SlopeOne()\n",
    "slopeOne.fit(trainset)\n",
    "dump.dump(file_name='modelli_Allenati/slopeOne',algo=slopeOne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased accuracy on trainset svd,   RMSE: 0.7348\n",
      "MSE: 0.5399\n",
      "MAE:  0.5833\n",
      "Unbiased accuracy on testset, RMSE: 0.8572\n",
      "MSE: 0.7347\n",
      "MAE:  0.6761\n",
      "precision: \n",
      "0.7514576789726016\n",
      "recall: \n",
      "0.3431329161442998\n"
     ]
    }
   ],
   "source": [
    "#risultati ottenuti svd\n",
    "\n",
    "# training errors\n",
    "biasSet=trainset.build_testset()\n",
    "predictions_svd_train = algo_svd.test(biasSet)\n",
    "print(\"Biased accuracy on trainset svd,\", end=\"   \")\n",
    "accuracy.rmse(predictions_svd_train)\n",
    "accuracy.mse(predictions_svd_train)\n",
    "accuracy.mae(predictions_svd_train)\n",
    "\n",
    "# test errors\n",
    "predictions_svd_test=algo_svd.test(testset)\n",
    "print(\"Unbiased accuracy on testset,\", end=\" \")\n",
    "accuracy.rmse(predictions_svd_test)\n",
    "accuracy.mse(predictions_svd_test)\n",
    "accuracy.mae(predictions_svd_test)\n",
    "\n",
    "precisions, recalls = precision_recall_at_k(predictions_svd_test, k=10, threshold=4)\n",
    "print(\"precision: \")\n",
    "print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "print(\"recall: \")\n",
    "print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased accuracy on trainset svd,   RMSE: 0.6789\n",
      "MSE: 0.4608\n",
      "MAE:  0.5128\n",
      "Unbiased accuracy on testset, RMSE: 0.9228\n",
      "MSE: 0.8515\n",
      "MAE:  0.7277\n",
      "precision: \n",
      "0.7851060919340725\n",
      "recall: \n",
      "0.3920950352170502\n"
     ]
    }
   ],
   "source": [
    "#risultati ottenuti Knn user-based\n",
    "\n",
    "# training errors\n",
    "predictions_KnnB_u_train = algo_knnBase_u.test(biasSet)\n",
    "print(\"Biased accuracy on trainset svd,\", end=\"   \")\n",
    "accuracy.rmse(predictions_KnnB_u_train)\n",
    "accuracy.mse(predictions_KnnB_u_train)\n",
    "accuracy.mae(predictions_KnnB_u_train)\n",
    "\n",
    "# test errors\n",
    "predictions_KnnB_u_test=algo_knnBase_u.test(testset)\n",
    "print(\"Unbiased accuracy on testset,\", end=\" \")\n",
    "accuracy.rmse(predictions_KnnB_u_test)\n",
    "accuracy.mse(predictions_KnnB_u_test)\n",
    "accuracy.mae(predictions_KnnB_u_test)\n",
    "\n",
    "precisions, recalls = precision_recall_at_k(predictions_KnnB_u_test, k=10, threshold=4)\n",
    "print(\"precision: \")\n",
    "print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "print(\"recall: \")\n",
    "print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased accuracy on trainset svd,   RMSE: 0.7599\n",
      "MSE: 0.5774\n",
      "MAE:  0.5898\n",
      "Unbiased accuracy on testset, RMSE: 0.9159\n",
      "MSE: 0.8388\n",
      "MAE:  0.7213\n",
      "precision: \n",
      "0.5221823686938085\n",
      "recall: \n",
      "0.27933974192074956\n"
     ]
    }
   ],
   "source": [
    "#risultati ottenuti Knn item-based\n",
    "\n",
    "# training errors\n",
    "predictions_KnnB_i_train = algo_knnBase_i.test(biasSet)\n",
    "print(\"Biased accuracy on trainset svd,\", end=\"   \")\n",
    "accuracy.rmse(predictions_KnnB_i_train)\n",
    "accuracy.mse(predictions_KnnB_i_train)\n",
    "accuracy.mae(predictions_KnnB_i_train)\n",
    "\n",
    "# test errors\n",
    "predictions_KnnB_i_test=algo_knnBase_i.test(testset)\n",
    "print(\"Unbiased accuracy on testset,\", end=\" \")\n",
    "accuracy.rmse(predictions_KnnB_i_test)\n",
    "accuracy.mse(predictions_KnnB_i_test)\n",
    "accuracy.mae(predictions_KnnB_i_test)\n",
    "\n",
    "precisions, recalls = precision_recall_at_k(predictions_KnnB_i_test, k=10, threshold=4)\n",
    "print(\"precision: \")\n",
    "print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "print(\"recall: \")\n",
    "print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased accuracy on trainset svd,   RMSE: 0.7132\n",
      "MSE: 0.5087\n",
      "MAE:  0.5540\n",
      "Unbiased accuracy on testset, RMSE: 0.9263\n",
      "MSE: 0.8579\n",
      "MAE:  0.7367\n",
      "precision: \n",
      "0.6255770212183909\n",
      "recall: \n",
      "0.3217260706326117\n"
     ]
    }
   ],
   "source": [
    "#risultati ottenuti Knn user-based\n",
    "\n",
    "# training errors\n",
    "predictions_KnnM_u_train = algo_knnMeans_u.test(biasSet)\n",
    "print(\"Biased accuracy on trainset svd,\", end=\"   \")\n",
    "accuracy.rmse(predictions_KnnM_u_train)\n",
    "accuracy.mse(predictions_KnnM_u_train)\n",
    "accuracy.mae(predictions_KnnM_u_train)\n",
    "\n",
    "# test errors\n",
    "predictions_KnnM_u_test=algo_knnMeans_u.test(testset)\n",
    "print(\"Unbiased accuracy on testset,\", end=\" \")\n",
    "accuracy.rmse(predictions_KnnM_u_test)\n",
    "accuracy.mse(predictions_KnnM_u_test)\n",
    "accuracy.mae(predictions_KnnM_u_test)\n",
    "\n",
    "precisions, recalls = precision_recall_at_k(predictions_KnnM_u_test, k=10, threshold=4)\n",
    "print(\"precision: \")\n",
    "print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "print(\"recall: \")\n",
    "print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased accuracy on trainset svd,   RMSE: 0.7501\n",
      "MSE: 0.5626\n",
      "MAE:  0.5860\n",
      "Unbiased accuracy on testset, RMSE: 0.8851\n",
      "MSE: 0.7835\n",
      "MAE:  0.6951\n",
      "precision: \n",
      "0.7243822703095227\n",
      "recall: \n",
      "0.3476985887485839\n"
     ]
    }
   ],
   "source": [
    "#risultati ottenuti Knn con media item-based\n",
    "\n",
    "# training errors\n",
    "predictions_KnnM_i_train = algo_knnMeans_i.test(biasSet)\n",
    "print(\"Biased accuracy on trainset svd,\", end=\"   \")\n",
    "accuracy.rmse(predictions_KnnM_i_train)\n",
    "accuracy.mse(predictions_KnnM_i_train)\n",
    "accuracy.mae(predictions_KnnM_i_train)\n",
    "\n",
    "# test errors\n",
    "predictions_KnnM_i_test=algo_knnMeans_i.test(testset)\n",
    "print(\"Unbiased accuracy on testset,\", end=\" \")\n",
    "accuracy.rmse(predictions_KnnM_i_test)\n",
    "accuracy.mse(predictions_KnnM_i_test)\n",
    "accuracy.mae(predictions_KnnM_i_test)\n",
    "\n",
    "precisions, recalls = precision_recall_at_k(predictions_KnnM_i_test, k=10, threshold=4)\n",
    "print(\"precision: \")\n",
    "print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "print(\"recall: \")\n",
    "print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased accuracy on trainset svd,   RMSE: 0.8695\n",
      "MSE: 0.7560\n",
      "MAE:  0.6852\n",
      "Unbiased accuracy on testset, RMSE: 0.9061\n",
      "MSE: 0.8210\n",
      "MAE:  0.7149\n",
      "precision: \n",
      "0.7173578557486462\n",
      "recall: \n",
      "0.3386317120869436\n"
     ]
    }
   ],
   "source": [
    "#risultati ottenuti SlopeOne\n",
    "\n",
    "# training errors\n",
    "predictions_slopeOne_train = slopeOne.test(biasSet)\n",
    "print(\"Biased accuracy on trainset svd,\", end=\"   \")\n",
    "accuracy.rmse(predictions_slopeOne_train)\n",
    "accuracy.mse(predictions_slopeOne_train)\n",
    "accuracy.mae(predictions_slopeOne_train)\n",
    "\n",
    "# test errors\n",
    "predictions_slopeOne_test=slopeOne.test(testset)\n",
    "print(\"Unbiased accuracy on testset,\", end=\" \")\n",
    "accuracy.rmse(predictions_slopeOne_test)\n",
    "accuracy.mse(predictions_slopeOne_test)\n",
    "accuracy.mae(predictions_slopeOne_test)\n",
    "\n",
    "precisions, recalls = precision_recall_at_k(predictions_slopeOne_test, k=10, threshold=4)\n",
    "print(\"precision: \")\n",
    "print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "print(\"recall: \")\n",
    "print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased accuracy on trainset,   RMSE: 0.6101\n",
      "MSE: 0.3722\n",
      "MAE:  0.4640\n",
      "Biased accuracy on trainset,   RMSE: 0.8975\n",
      "MSE: 0.8054\n",
      "MAE:  0.7032\n",
      "precision: \n",
      "0.7672248791241224\n",
      "recall: \n",
      "0.37819100013350215\n"
     ]
    }
   ],
   "source": [
    "#risultati ottenuti Knn ibrido\n",
    "\n",
    "prediction_ibridoB_train=predictIbrido(algo_KnnBase_ibrido,algo_knnBase_u,algo_knnBase_i,biasSet)\n",
    "print(\"Biased accuracy on trainset,\", end=\"   \")\n",
    "accuracy.rmse(prediction_ibridoB_train)\n",
    "accuracy.mse(prediction_ibridoB_train)\n",
    "accuracy.mae(prediction_ibridoB_train)\n",
    "\n",
    "prediction_ibridoB_test=predictIbrido(algo_KnnBase_ibrido,algo_knnBase_u,algo_knnBase_i,testset)\n",
    "print(\"Biased accuracy on trainset,\", end=\"   \")\n",
    "accuracy.rmse(prediction_ibridoB_test)\n",
    "accuracy.mse(prediction_ibridoB_test)\n",
    "accuracy.mae(prediction_ibridoB_test)\n",
    "\n",
    "precisions, recalls = precision_recall_at_k(prediction_ibridoB_test, k=10, threshold=4)\n",
    "print(\"precision: \")\n",
    "print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "print(\"recall: \")\n",
    "print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased accuracy on trainset,   RMSE: 0.6898\n",
      "MSE: 0.4759\n",
      "MAE:  0.5353\n",
      "Biased accuracy on trainset,   RMSE: 0.9093\n",
      "MSE: 0.8268\n",
      "MAE:  0.7129\n",
      "precision: \n",
      "0.7057435774000307\n",
      "recall: \n",
      "0.3766595927917136\n"
     ]
    }
   ],
   "source": [
    "#risultati ottenuti Knn con media ibrido \n",
    "\n",
    "prediction_ibridoM_train=predictIbrido(algo_KnnMeans_ibrido,algo_knnMeans_u,algo_knnMeans_i,biasSet)\n",
    "print(\"Biased accuracy on trainset,\", end=\"   \")\n",
    "accuracy.rmse(prediction_ibridoM_train)\n",
    "accuracy.mse(prediction_ibridoM_train)\n",
    "accuracy.mae(prediction_ibridoM_train)\n",
    "\n",
    "prediction_ibridoM_test=predictIbrido(algo_KnnMeans_ibrido,algo_knnMeans_u,algo_knnMeans_i,testset)\n",
    "print(\"Biased accuracy on trainset,\", end=\"   \")\n",
    "accuracy.rmse(prediction_ibridoM_test)\n",
    "accuracy.mse(prediction_ibridoM_test)\n",
    "accuracy.mae(prediction_ibridoM_test)\n",
    "\n",
    "precisions, recalls = precision_recall_at_k(prediction_ibridoM_test, k=10, threshold=4)\n",
    "print(\"precision: \")\n",
    "print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "print(\"recall: \")\n",
    "print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_11_7_uniroma3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
